# Safety & Guardrails System

## Overview
This project demonstrates how prompt engineering can be used
to prevent harmful, incorrect, or overconfident AI outputs.

The focus is on:
- Scope control
- Uncertainty handling
- Safe refusal and redirection
- Responsible AI behavior

## Guardrails Included
- Scope Limiter
- Uncertainty Handler
- Refusal & Redirection
- Safe Alternative Generator

## Examples

### Scope Limiter
![Scope Limiter](screenshots/scope-limiter.png)

### Uncertainty Handler
![Uncertainty Handler](screenshots/uncertainty-handler.png)

### Refusal & Redirection
![Refusal](screenshots/refusal.png)

### Safe Alternative Generator
![Safe Alternative](screenshots/safe-alternative.png)

## Why This Matters
Reliable AI systems must know when **not** to answer.
This project shows how prompt design can enforce safety,
transparency, and user trust.
